{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Libraries </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime\n",
    "!pip install shap\n",
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gc\n",
    "import pickle\n",
    "#preprocessing and feature selection\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "#models\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#evaluation and interpretability\n",
    "import lime\n",
    "import shap\n",
    "import eli5\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pre-process Data for Model Training </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1: Encode Datasets <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firm                object\n",
       "Sector              object\n",
       "DistanceFromLast    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata = pd.read_pickle('../data/sp500finaldata.pkl')\n",
    "#The only features that need to be label encoded are 'Sector' and 'DistanceFromLast'\n",
    "alldata.dtypes[alldata.dtypes=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 4 4 4]\n",
      "[6 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "labenc = LabelEncoder()\n",
    "labenc.fit(alldata['Sector'])\n",
    "with open('labencsector.pkl', 'wb') as q:\n",
    "    pickle.dump(labenc, q)\n",
    "print(labenc.transform(alldata['Sector']))\n",
    "alldata['Sector'] = labenc.transform(alldata['Sector'])\n",
    "labenc1 = LabelEncoder()\n",
    "labenc1.fit(alldata['DistanceFromLast'])\n",
    "with open('labencdist.pkl', 'wb') as q:\n",
    "    pickle.dump(labenc1, q)\n",
    "print(labenc1.transform(alldata['DistanceFromLast']))\n",
    "alldata['DistanceFromLast'] = labenc1.transform(alldata['DistanceFromLast'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "#Since we would be dropping the 'firm' feature, only 'Sector' and 'DistanceFromLast' from above need to be hot-encoded\n",
    "onehot.fit(alldata[['Sector', 'DistanceFromLast']])\n",
    "with open('onehotenc.pkl', 'wb') as q:\n",
    "    pickle.dump(onehot, q)\n",
    "print(onehot.transform(alldata[['Sector', 'DistanceFromLast']]))\n",
    "namessector = ['Sector_'+str(i) for i in labenc.classes_]\n",
    "namesdist = ['DistLast_'+str(i) for i in labenc1.classes_]\n",
    "names = np.append(namessector, namesdist)\n",
    "enc = onehot.transform(alldata[['Sector', 'DistanceFromLast']])\n",
    "enc = pd.DataFrame(enc, columns=names)\n",
    "alldata.drop(['firm', 'Sector', 'DistanceFromLast'], axis=1, inplace=True)\n",
    "alldata = pd.concat([alldata, enc], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 2 - Separate Two Models </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide our project into two distinct models - one where we attempt to predict 'Div_Paid?' (hence a classifier model predicting whether or not dividends will be paid), and another where we attempt to predict how much dividend, if paid (therefore a regressor model). Let us designate the different datasets for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifdata = alldata.copy()\n",
    "classifdata.drop(['Dividend', 'Dividend Yield', 'Payout Ratio'], axis=1, inplace=True)\n",
    "regrdata = alldata[alldata['Div_Paid?']==1].copy()\n",
    "regrdata.drop(['Div_Paid?','Dividend Yield', 'Payout Ratio'], axis=1, inplace=True)\n",
    "classifdata.to_pickle('../data/classifdatafull.pkl')\n",
    "regrdata.to_pickle('../data/regrdatafull.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Classifier Model - Dividend Outlook </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we work towards the classifier model, and begin with feature selection. Subsequently, we try three different fits - RandomForestClassifier, GaussianProcessClassifier and MLPClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1 - Feature Elimination </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform basic feature elimination here, and leave actual selection to fit based on model. The only selection performed here is chi2 testing (to check relationship to the target variable) and VarianceThreshold (differences within the feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9, 122\n"
     ]
    }
   ],
   "source": [
    "#The simple variance elimination conducted on all variables. A generic 0.001 variance cut-off is used, since such a\n",
    "#lax value allows us to include a multitude of variables whilst excluding those that do not significantly vary\n",
    "scaler = MinMaxScaler(feature_range=(0,1), copy=True)\n",
    "scld = scaler.fit_transform(classifdata)\n",
    "scld = pd.DataFrame(scld, columns=classifdata.columns.values)\n",
    "slt = VarianceThreshold(0.001)\n",
    "slt.fit(scld)\n",
    "lowvar = classifdata.columns.values[slt.get_support(indices=False)==False]\n",
    "goodvar = list(set(classifdata.columns.values) - set(lowvar))\n",
    "print(str(len(lowvar)) + \", \" + str(len(goodvar)))\n",
    "classifdata.drop(lowvar, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We subsequently proceed to chi square analysis to determine which features do not truly vary with the response var\n",
    "#Since 'other income (expense)' has negative values, we ignore it since chi2 needs positive values for calculations\n",
    "#To reject the null hypothesis and claim that there is truly a relationship between the feature and response, we set \n",
    "#a chi-square cut-off at 4 (1 df approx value).\n",
    "chi2arr, pval = chi2(classifdata.drop(['Div_Paid?','Other income (expense)'], axis=1), classifdata['Div_Paid?'])\n",
    "lowrel = classifdata.drop(['Div_Paid?','Other income (expense)'], axis=1).columns.values[chi2arr < 4]\n",
    "classifdata.drop(lowrel, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 2 - Separate Training and Test Data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use train-test split to decide the training and test data\n",
    "Train_x, Test_x, Train_y, Test_y = train_test_split(classifdata.drop(['Div_Paid?'], axis=1), classifdata['Div_Paid?'], random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 3 - RandomForestClassifier Implementation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#We set up a randomizedsearchCV to perform hyperparameter tuning on a random forest classifier\n",
    "params = {'n_estimators':[300, 500, 700], 'min_samples_split':[2,4,5], 'min_samples_leaf':[1,2,3], \n",
    "          'max_depth':[10, None], 'max_features':['auto', 'log2']}\n",
    "mdl = RandomForestClassifier(n_jobs=-1, random_state=1)\n",
    "randforcf = RandomizedSearchCV(estimator=mdl, param_distributions=params, n_jobs=-1, n_iter=10, cv=3, random_state=1)\n",
    "randforcf.fit(Train_x, Train_y)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=-1, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Refit random forest on the best estimators gained\n",
    "paramdict = randforcf.best_params_\n",
    "randforcf = RandomForestClassifier(n_jobs=-1, random_state=1, n_estimators=paramdict['n_estimators'], min_samples_split=paramdict['min_samples_split'], min_samples_leaf=paramdict['min_samples_leaf'], max_features=paramdict['max_features'], max_depth=paramdict['max_depth'])\n",
    "randforcf.fit(Train_x, Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#As before, we use a 0.001 cut-off for better fit. Using feature_importances_, we retrieve features with lower\n",
    "#importance and eliminate them\n",
    "elim=pd.Series(randforcf.feature_importances_, index=Train_x.columns)\n",
    "elim = elim[elim < 0.001].index\n",
    "Train_x.drop(elim, axis=1, inplace=True)\n",
    "Test_x.drop(elim, axis=1, inplace=True)\n",
    "randforcf = RandomForestClassifier(n_jobs=-1, random_state=1, n_estimators=paramdict['n_estimators'], min_samples_split=paramdict['min_samples_split'], min_samples_leaf=paramdict['min_samples_leaf'], max_features=paramdict['max_features'], max_depth=paramdict['max_depth'])\n",
    "randforcf.fit(Train_x, Train_y)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789674952198852"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform sklearn metrics evaluation\n",
    "ypred = randforcf.predict(Test_x)\n",
    "sklearn.metrics.accuracy_score(Test_y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
