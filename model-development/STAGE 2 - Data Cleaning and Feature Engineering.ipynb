{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h2> Import Requirements </h2>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip3 install impyute\n!pip install -U scikit-learn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: impyute in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (0.0.8)\nRequirement already satisfied: numpy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from impyute) (1.16.3)\nRequirement already satisfied: scikit-learn in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from impyute) (0.21.2)\nRequirement already satisfied: scipy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from impyute) (1.1.0)\nRequirement already satisfied: joblib>=0.11 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn->impyute) (0.12.5)\n\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nRequirement already up-to-date: scikit-learn in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (0.21.2)\nRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (1.16.3)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (0.12.5)\nRequirement already satisfied, skipping upgrade: scipy>=0.17.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (1.1.0)\n\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nfrom impyute.imputation.cs import fast_knn\nfrom impyute.imputation.cs import mice",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h2> Load Necessary Data </h2>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata = pd.read_csv(\"sp500alldata.csv\", error_bad_lines=False)\nalldata.info()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2566 entries, 0 to 2565\nColumns: 132 entries, date to firm\ndtypes: float64(130), object(2)\nmemory usage: 2.6+ MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "constdata = pd.read_csv(\"constituents_csv.csv\", error_bad_lines=False)\nconstdata.info()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 505 entries, 0 to 504\nData columns (total 3 columns):\nSymbol    505 non-null object\nName      505 non-null object\nSector    505 non-null object\ndtypes: object(3)\nmemory usage: 11.9+ KB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "econdata = pd.read_csv(\"annualecondata.csv\", error_bad_lines=False)\necondata.info()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 58 entries, 0 to 57\nData columns (total 4 columns):\nDate       58 non-null int64\nValue_x    58 non-null float64\nValue_y    58 non-null float64\nValue      43 non-null float64\ndtypes: float64(3), int64(1)\nmemory usage: 1.9 KB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "stockconf = pd.read_csv(\"stockconfidencedata.csv\", error_bad_lines=False)\nstockconf.info()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 221 entries, 0 to 220\nData columns (total 3 columns):\nDate              221 non-null int64\nIndex Value       221 non-null float64\nStandard Error    221 non-null float64\ndtypes: float64(2), int64(1)\nmemory usage: 5.3 KB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h2> Prepare Consolidated Dataset </h2>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 1: Dimension Cleaning </b>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "While most of the data is 5-year worth of yearly information, some errenous entries have far more frequent information - monthly, quarterly etc., owing to irregularities in data collection. Since dividend payments are considered yearly, such errenous values must be dealt with"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata.sort_values(by=['firm', 'date'], ascending = [True, True], inplace=True)\nalldata.reset_index(drop=True, inplace=True)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata.date.apply(lambda s: 'VALID' if str(s)[0].isnumeric() else s).value_counts(dropna=False)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "VALID         2560\nUnnamed: 1       6\nName: date, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "firms = alldata[alldata.date=='Unnamed: 1'].firm\nalldata = alldata[(~(alldata.firm.isin(firms)))]",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "counter = alldata.date.apply(lambda s: int(s.split(\"-\")[0]))\nrmv = alldata.loc[(counter.diff().shift(-1)==0) & (alldata.firm == alldata.firm.shift(-1)), ['date', 'firm']]\nrmv.firm.unique()",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "array(['EVHC', 'HBI', 'NFX', 'NWS', 'SCG', 'UA', 'YUM'], dtype=object)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata.drop(rmv.index, inplace=True)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "<b> STEP 2: Missing Value Deletions </b>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We first analyze the number of missing values across columns and decide to drop any above 60%. As seen below, 21 of the 168 columns meet this requirement. Out of these, however, investments in research and development is considered an important factor for a firm, and is hence kept."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = alldata.apply(lambda s: 1 if (sum(s.isnull() * 1)) > 0.6 * len(s) else np.nan).dropna()\nx",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "Operating expenses                      1.0\nResearch and development                1.0\nOther                                   1.0\nEarnings per share                      1.0\nWeighted average shares outstanding     1.0\nAssets                                  1.0\nCurrent assets                          1.0\nCash                                    1.0\nShort-term investments                  1.0\nNon-current assets                      1.0\nProperty, plant and equipment           1.0\nEquity and other investments            1.0\nPrepaid pension benefit                 1.0\nLiabilities and stockholders' equity    1.0\nLiabilities                             1.0\nCurrent liabilities                     1.0\nNon-current liabilities                 1.0\nCapital leases                          1.0\nPensions and other benefits             1.0\nMinority interest                       1.0\nStockholders' equity                    1.0\ndtype: float64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "coldel = np.delete(x.index.values, np.where(x.index.values=='Research and development')[0])\nalldata.drop(coldel, axis=1, inplace=True)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#We now analyze the number of null values per company. While most companies have 5 rows (5 years), some have fewer and so we need to make\n#sure that the firms where np.nan exceeds 60% of values are removed. We can use value_counts() to retrieve how many rows a firm has\nrownum = alldata.firm.value_counts()\nrownum = rownum[alldata.firm.unique()]\n#Having retrived correct row counts, we now get null numbers for each company\nnullnum = alldata.apply(lambda s: sum(s.isnull() * 1), axis=1)\nnullnum = np.cumsum(nullnum)\nchangeorg = nullnum.where(alldata.firm != alldata.firm.shift(1)).ffill().fillna(0)\nnullnum = nullnum - changeorg\nnullnum = nullnum.where(alldata.firm != alldata.firm.shift(-1)).dropna()\nnullnum.index = alldata.firm.unique()\nnval = nullnum[nullnum >= 0.6 * rownum * 148]\nnval",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "NFX    535.0\nNWS    535.0\ndtype: float64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "firmstodrop = nval.index.values\nindices = alldata[alldata.firm.isin(firmstodrop)].index.values\nalldata.drop(indices, axis=0, inplace=True)\nalldata.reset_index(drop=True, inplace=True)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Special adjustment based on quality, relevance and repetition of data involved\n#The columns involved below are  repetitions or correlations of existing financial metrics, thus crowding analysis\nbadcols = ['Net income from continuing operations', 'Net income available to common shareholders', \n           'Cash and cash equivalents', 'Current ratio','Debt to Equity','ROE', 'ROIC', \n           'Receivables Turnover','Return on Tangible Assets', \n           'Tangible Asset Value','Working Capital']\nalldata.drop(badcols, axis=1, inplace=True)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 3: Merge Supplementary Data </b>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We must combine the main S&P500 financial dataset with the economics, stock confidence and sector data that was collected simultaneously. Sector data is merged on firm, annual economics on year and stock confidence on month. Relevant features are temporarily added to facilitate such merges."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "constdata.rename(columns={'Symbol':'firm'}, inplace=True)\nalldata = alldata.merge(constdata[['firm', 'Sector']], on='firm', how='left')",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cols = econdata.columns.values\nvals = ['Year','GDP', 'GNI', 'Market_Cap']\necondata.rename(columns = dict(zip(cols, vals)), inplace=True)\necondata.Year = econdata.Year.apply(lambda s: int(s/100))\nalldata['Year'] = alldata.date.apply(lambda s: int(s.split('-')[0]))",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Front fill to latest economics entry required before merging\nlatestecon = econdata.sort_values(by='Year', ascending=False).iloc[0]\ncurtime = alldata.Year.max()\nnewvals = econdata.loc[np.array([0]).repeat(curtime - latestecon.Year)].copy()\nadj = pd.Series(np.arange(curtime - latestecon.Year) + 1)\nnewvals.reset_index(drop=True, inplace=True)\nnewvals['Year'] = newvals['Year'] + adj\necondata = econdata.append(newvals)\necondata.sort_values(by='Year', ascending=False, inplace=True)\necondata = econdata[econdata.Year >= alldata.Year.min()]\necondata.reset_index(drop=True, inplace=True)",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "econdata.Year = econdata.Year.astype('int')\nalldata = alldata.merge(econdata, on='Year', how='left')\nalldata.drop(['Year'], axis=1, inplace=True)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def yearshift(base, amt):\n    mth = base % 100\n    year = (base - mth)/100\n    mth = mth + amt\n    if mth > 12:\n        year = year + (mth - (mth % 12))/12\n        mth = mth % 12\n    return int(year * 100 + mth)",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def mthdiff (val1, val2):\n    endyear = int(val1/100)\n    endmonth = val1 % 100\n    startyear = int(val2/100)\n    startmonth = val2 % 100\n    return (endyear - startyear) * 100 + (endmonth - startmonth)",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Front fill to latest stock confidence values required before merging\nstockconf.rename(columns={'Date':'date'}, inplace=True)\nalldata.date = alldata.date.apply(lambda s: int(s.split('-')[0] + s.split('-')[1]))\nconflatest = stockconf.date.max()\ncurperiod = alldata.date.max()\nnewvals = stockconf.loc[np.array([stockconf.loc[stockconf.date==conflatest].index[0]]).repeat(mthdiff(curperiod, conflatest))]\nnewvals.reset_index(drop=True, inplace=True)\nadj = pd.Series(np.arange(mthdiff(curperiod, conflatest)) + 1)\nnewvals['date'] = newvals['date'].astype('str') + ' ' + adj.astype('str')\nnewvals['date'] = newvals['date'].apply(lambda s: yearshift(int(s.split()[0]), int(s.split()[1])))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "stockconf = stockconf.append(newvals)\nstockconf.sort_values(by='date', ascending=False)\nstockconf = stockconf[stockconf.date >= alldata.date.min()]\nstockconf.reset_index(drop=True, inplace=True)\nalldata = alldata.merge(stockconf, on='date', how='left')\nalldata.info()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2324 entries, 0 to 2323\nColumns: 107 entries, date to Standard Error\ndtypes: float64(104), int64(1), object(2)\nmemory usage: 1.9+ MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h2> Missing Value Imputation </h2>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The strategy for imputation we decided to adopt was three-pronged. First, we attempt to calculate income and balance sheet values (not ratios), if possible from other corresponding columns, like Revenue = Grossprofit + Cost of revenue, etc. We repeat this for income statement and balance sheet variables (since such corrections exist), until no better values can be figured. For remaining null values, in the second stage, we fill all nulls in a column with a company-by-company average of non-null figures. In the third stage, if such missing values continue to persist, we  impute them with  average value from their sector of operations. Finally, we re-calculate all ratios from the new information gained by performing the three stages on income/balance sheet data, and any ratio outside the scope of such calculation is filled again by sklearn's Imputer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "<b> STEP 1: Internal Calculations </b>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Income statement variables\n#We repeat it till it is able to make no more difference\noldnullinc = np.inf\ncurnullinc = alldata.apply(lambda s: sum(s.isnull()))[:13].values.sum()\nwhile curnullinc < oldnullinc:\n    alldata.loc[alldata.Revenue.isnull(), \"Revenue\"] = alldata[\"Gross profit\"] + alldata[\"Cost of revenue\"]\n    alldata.loc[alldata['Cost of revenue'].isnull(), \"Cost of revenue\"] = alldata[\"Revenue\"] - alldata[\"Gross profit\"]\n    alldata.loc[alldata['Gross profit'].isnull(), \"Gross profit\"] = alldata[\"Revenue\"] - alldata[\"Cost of revenue\"]\n    alldata.loc[alldata['Gross profit'].isnull(), \"Gross profit\"] = alldata[\"Operating income\"] + alldata[\"Total operating expenses\"]\n    alldata.loc[alldata['Total operating expenses'].isnull(), \"Total operating expenses\"] = alldata[\"Gross profit\"] - alldata[\"Operating income\"]\n    alldata.loc[alldata['Operating income'].isnull(), \"Operating income\"] = alldata[\"Gross profit\"] - alldata[\"Total operating expenses\"]\n    alldata.loc[alldata['Operating income'].isnull(), \"Operating income\"]  = alldata[\"Income before taxes\"] - alldata['Other income (expense)'] + alldata['Interest Expense']\n    alldata.loc[alldata['Interest Expense'].isnull(), \"Interest Expense\"] = alldata['Operating income'] + alldata['Other income (expense)'] - alldata['Income before taxes']\n    alldata.loc[alldata['Other income (expense)'].isnull(), \"Other income (expense)\"] = alldata[\"Income before taxes\"] + alldata[\"Interest Expense\"] - alldata[\"Operating income\"]\n    alldata.loc[alldata['Income before taxes'].isnull(), \"Income before taxes\"] = alldata['Operating income'] - alldata['Interest Expense'] + alldata['Other income (expense)']\n    alldata.loc[alldata['Income before taxes'].isnull(), \"Income before taxes\"] = alldata['Net income'] + alldata['Provision for income taxes']\n    alldata.loc[alldata['Provision for income taxes'].isnull(), \"Provision for income taxes\"] = alldata[\"Income before taxes\"] - alldata[\"Net income\"]\n    alldata.loc[alldata['Net income'].isnull(), \"Net income\"] = alldata['Income before taxes'] - alldata['Provision for income taxes']\n    oldnullinc = curnullinc\n    curnullinc = alldata.apply(lambda s: sum(s.isnull()))[:13].values.sum()\n    print(curnullinc)\n\nprint(\"Income values done\")",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "8544\n8347\n8347\nIncome values done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Balance sheet variables\n#We repeat till it is able to make no more difference, similarly as above\noldnullinc = np.inf\ncurnullinc = alldata.apply(lambda s: sum(s.isnull()))[16:48].values.sum()\nwhile curnullinc < oldnullinc:\n    alldata.loc[alldata['Total current assets'].isnull(), \"Total current assets\"] = alldata['Total cash'] + alldata['Receivables'] + alldata['Inventories'] + alldata['Prepaid expenses'] + alldata['Other current assets']\n    alldata.loc[alldata['Total cash'].isnull(), \"Total cash\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Inventories'] - alldata['Prepaid expenses'] - alldata['Other current assets']\n    alldata.loc[alldata['Receivables'].isnull(), \"Receivables\"] = alldata['Total current assets'] - alldata['Total cash'] - alldata['Inventories'] - alldata['Prepaid expenses'] - alldata['Other current assets']\n    alldata.loc[alldata['Inventories'].isnull(), \"Inventories\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Total cash'] - alldata['Prepaid expenses'] - alldata['Other current assets']\n    alldata.loc[alldata['Prepaid expenses'].isnull(), \"Prepaid expenses\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Inventories'] - alldata['Total cash'] - alldata['Other current assets']\n    alldata.loc[alldata['Other current assets'].isnull(), \"Other current assets\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Inventories'] - alldata['Prepaid expenses'] - alldata['Total cash']\n    alldata.loc[alldata['Gross property, plant and equipment'].isnull(), \"Gross property, plant and equipment\"] = alldata['Net property, plant and equipment'] - alldata['Accumulated Depreciation']\n    alldata.loc[alldata['Accumulated Depreciation'].isnull(), \"Accumulated Depreciation\"] = alldata['Net property, plant and equipment'] - alldata['Gross property, plant and equipment']\n    alldata.loc[alldata['Net property, plant and equipment'].isnull(), 'Net property, plant and equipment'] = alldata['Gross property, plant and equipment'] + alldata['Accumulated Depreciation']\n    alldata.loc[alldata['Net property, plant and equipment'].isnull(), 'Net property, plant and equipment'] = alldata['Total non-current assets'] - alldata['Goodwill'] - alldata['Intangible assets'] - alldata['Other long-term assets']\n    alldata.loc[alldata['Goodwill'].isnull(), \"Goodwill\"] = alldata['Total non-current assets'] - alldata['Net property, plant and equipment'] - alldata['Intangible assets'] - alldata['Other long-term assets']\n    alldata.loc[alldata['Intangible assets'].isnull(), \"Intangible assets\"] = alldata['Total non-current assets'] - alldata['Goodwill'] - alldata['Net property, plant and equipment'] - alldata['Other long-term assets']\n    alldata.loc[alldata['Other long-term assets'].isnull(), \"Other long-term assets\"] = alldata['Total non-current assets'] - alldata['Goodwill'] - alldata['Intangible assets'] - alldata['Net property, plant and equipment']\n    alldata.loc[alldata['Total non-current assets'].isnull(), \"Total non-current assets\"] = alldata['Net property, plant and equipment'] + alldata['Goodwill'] + alldata['Intangible assets'] + alldata['Other long-term assets']\n    alldata.loc[alldata['Total assets'].isnull(), 'Total assets']  = alldata['Total current assets'] + alldata['Total non-current assets']\n    alldata.loc[alldata['Total current assets'].isnull(), \"Total current assets\"] = alldata['Total assets'] - alldata['Total non-current assets']\n    alldata.loc[alldata['Total non-current assets'].isnull(), 'Total non-current assets'] = alldata['Total assets'] - alldata['Total current assets']\n    \n    alldata.loc[alldata['Short-term debt'].isnull(), \"Short-term debt\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Accrued liabilities'] - alldata['Taxes payable'] - alldata['Accounts payable']\n    alldata.loc[alldata['Accounts payable'].isnull(), \"Accounts payable\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Accrued liabilities'] - alldata['Taxes payable'] - alldata['Short-term debt']\n    alldata.loc[alldata['Taxes payable'].isnull(), \"Taxes payable\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Accrued liabilities'] - alldata['Short-term debt'] - alldata['Accounts payable']\n    alldata.loc[alldata['Accrued liabilities'].isnull(), \"Accrued liabilities\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Short-term debt'] - alldata['Taxes payable'] - alldata['Accounts payable']\n    alldata.loc[alldata['Other current liabilities'].isnull(), \"Other current liabilities\"] = alldata['Total current liabilities'] - alldata['Short-term debt'] - alldata['Accrued liabilities'] - alldata['Taxes payable'] - alldata['Accounts payable']\n    alldata.loc[alldata['Total current liabilities'].isnull(), \"Total current liabilities\"] = alldata['Short-term debt'] + alldata['Other current liabilities'] + alldata['Accrued liabilities'] + alldata['Taxes payable'] + alldata['Accounts payable']\n    alldata.loc[alldata['Long-term debt'].isnull(), \"Long-term debt\"] = alldata['Total non-current liabilities'] - alldata['Other long-term liabilities'] - alldata['Deferred taxes liabilities']\n    alldata.loc[alldata['Deferred taxes liabilities'].isnull(), \"Deferred taxes liabilities\"] = alldata['Total non-current liabilities'] - alldata['Other long-term liabilities'] - alldata['Long-term debt']\n    alldata.loc[alldata['Other long-term liabilities'].isnull(), \"Other long-term liabilities\"] = alldata['Total non-current liabilities'] -alldata['Long-term debt'] - alldata['Deferred taxes liabilities']\n    alldata.loc[alldata['Total non-current liabilities'].isnull(), \"Total non-current liabilities\"] = alldata['Long-term debt'] + alldata['Other long-term liabilities'] + alldata['Deferred taxes liabilities']\n    alldata.loc[alldata['Total liabilities'].isnull(), \"Total liabilities\"]  = alldata['Total current liabilities'] + alldata['Total non-current liabilities']\n    alldata.loc[alldata['Total current liabilities'].isnull(), \"Total current liabilities\"] = alldata[\"Total liabilities\"] - alldata['Total non-current liabilities']\n    alldata.loc[alldata['Total non-current liabilities'].isnull(), \"Total non-current liabilities\"] = alldata['Total liabilities'] - alldata['Total current liabilities']\n    \n    alldata.loc[alldata['Common stock'].isnull(), \"Common stock\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Treasury stock'] - alldata['Retained earnings'] - alldata['Additional paid-in capital']\n    alldata.loc[alldata['Additional paid-in capital'].isnull(), \"Additional paid-in capital\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Treasury stock'] - alldata['Retained earnings'] - alldata['Common stock']\n    alldata.loc[alldata['Retained earnings'].isnull(), \"Retained earnings\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Treasury stock'] - alldata['Common stock'] - alldata['Additional paid-in capital']\n    alldata.loc[alldata['Treasury stock'].isnull(), \"Treasury stock\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Common stock'] - alldata['Retained earnings'] - alldata['Additional paid-in capital']\n    alldata.loc[alldata['Accumulated other comprehensive income'].isnull(), \"Accumulated other comprehensive income\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Common stock\"] - alldata['Treasury stock'] - alldata['Retained earnings'] - alldata['Additional paid-in capital']\n    alldata.loc[alldata[\"Total stockholders' equity\"].isnull(), \"Total stockholders' equity\"] = alldata[\"Common stock\"] + alldata[\"Accumulated other comprehensive income\"] + alldata['Treasury stock'] + alldata['Retained earnings'] + alldata['Additional paid-in capital']\n    \n    alldata.loc[alldata[\"Total liabilities and stockholders' equity\"].isnull(), \"Total liabilities and stockholders' equity\"] = alldata['Total liabilities'] + alldata[\"Total stockholders' equity\"]\n    alldata.loc[alldata[\"Total stockholders' equity\"].isnull(), \"Total stockholders' equity\"] = alldata[\"Total liabilities and stockholders' equity\"] - alldata['Total liabilities']\n    alldata.loc[alldata['Total liabilities'].isnull(), \"Total liabilities\"] = alldata[\"Total liabilities and stockholders' equity\"] - alldata[\"Total stockholders' equity\"]\n    alldata.loc[alldata[\"Total assets\"].isnull(), \"Total assets\"] = alldata[\"Total liabilities and stockholders' equity\"]\n    alldata.loc[alldata[\"Total liabilities and stockholders' equity\"].isnull(), \"Total liabilities and stockholders' equity\"] = alldata[\"Total assets\"]\n    \n    oldnullinc = curnullinc\n    curnullinc = alldata.apply(lambda s: sum(s.isnull()))[16:48].values.sum()\n    print (curnullinc)\n\nprint(\"balance sheet variables done\")",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "18661\n18559\n18559\nbalance sheet variables done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#If any incorrect values (eg. negative stockholder's equity, liabilities > assets etc., these are set to np.nan, and left for imputation)\nnonneg = set(alldata.columns.values) - set(['Other income (expense)', 'firm', 'Sector'])\nfor col in nonneg:\n    alldata.loc[alldata[col] < 0, col] = np.nan\n#Special edge cases\nalldata.loc[alldata['Cost of revenue'] > alldata['Revenue'], ['Cost of revenue', 'Revenue', 'Gross profit']] = np.nan\nalldata.loc[alldata['Total assets'] < alldata['Total liabilities'], ['Total assets', 'Total liabilities']] = np.nan\nalldata.loc[alldata['Total current liabilities'] > alldata['Total liabilities'], ['Total current liabilities', 'Total non-current liabilities', 'Total liabilities']] = np.nan\nalldata.loc[alldata['Total non-current liabilities'] > alldata['Total liabilities'], ['Total current liabilities', 'Total non-current liabilities', 'Total liabilities']] = np.nan\n",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Other calculations\nalldata.loc[alldata.Basic_EPS.isnull(), \"Basic_EPS\"] = alldata[\"Diluted_EPS\"]\nalldata.loc[alldata.Diluted_EPS.isnull(), \"Diluted_EPS\"] = alldata[\"Basic_EPS\"]\nalldata.loc[alldata.EBITDA.isnull(), \"EBITDA\"] = alldata[\"Income before taxes\"] + alldata[\"Interest Expense\"]",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 2: Average Value Imputation - Historic Company Values and Sector Values </b>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Historic Company Averages\ndef correct(row, col, vals):\n    if (str(row[col])=='nan') & (vals[row['firm']] != 0):\n        row[col] = vals[row['firm']]\n    return row\n\ndef histcompavg (var):\n    if np.where(alldata.columns.values==var.name)[0][0] in range(1,48):\n        rev = np.cumsum(var).ffill()\n        rev = rev - (rev.where(alldata.firm != alldata.firm.shift(-1))).shift(1).ffill().fillna(0)\n        rev = rev[alldata.firm != alldata.firm.shift(-1)]\n        rev.index = alldata.firm.unique()\n        rownum = alldata.firm.value_counts()\n        rownum = rownum[alldata.firm.unique()]\n        rev = rev / rownum\n        x = pd.concat([var, alldata.firm], axis=1)\n        x = x.apply(lambda s: correct(s, var.name, rev), axis=1)\n        x[var.name] = x[var.name].apply(lambda s: round(s,2))\n        return x[var.name]\n    else:\n        return var\n\nalldata = alldata.apply(lambda s: histcompavg(s))",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Analyze sector composition\nalldata.Sector.value_counts(dropna=False)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "Consumer Discretionary        342\nIndustrials                   329\nFinancials                    329\nInformation Technology        310\nHealth Care                   292\nReal Estate                   153\nEnergy                        152\nConsumer Staples              143\nUtilities                     140\nMaterials                     119\nTelecommunication Services     15\nName: Sector, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Derive sector averages\n#We calculate sector averages below for all income and balance sheet variables. In addition, we need to handle 3 stock variables.\n#We have exactly two firms - FOX and FOXA (Consumer Discretionary) - that do not have any stock price recordings. Given that all these\n#companies are S&P500, it is safe to assume that companies in the same industry will average out around the same stock price\n#Moreover, only 7-8 recordings are affected by the same.\ndef sectavg (var):\n    if np.where(alldata.columns.values==var.name)[0][0] in np.append(np.arange(48)+1, np.array([97,98,99])):\n        industries = alldata.loc[var.isnull(), \"Sector\"].dropna().unique()\n        for ind in industries:\n            comm = np.sum(alldata.loc[alldata.Sector==ind, var.name])/ len(alldata.loc[alldata.Sector==ind, var.name])\n            var[(alldata.Sector==ind) & var.isnull()] = round(comm, 2)\n    return var\n\nalldata = alldata.apply(lambda s: sectavg(s))",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 3: Algorithmic Imputers </b>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After having eliminated all missing values in balance and income statements, we need algorithmic imputers - scikitlearn's preprocessing functions or impyute's knn and mice systems, to fill the remainder of values, before proceeding to feature engineering. For this final stage of data cleaning, we divide our features into 4 buckets - noact (no imputation; a fillna to be called), calc (ratios that can easily be calculated from other completed values), knnalg (ratios that cant be calculated but are closely linked to income/balance sheet values and so nearest neighbours is an ideal approach), and simimp (ratios that are finally not closely linked to available values and must be directly imputed)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Declare the final method of imputation for key metrics variables, mostly existing imputation algorithms\n#These variables determine the response indicator and thus cannot be imputed\nnoact = ['Dividend Yield', 'Payout Ratio']\n#These are calculated using known financial formulae\ncalc = ['Net Debt to EBITDA', 'SG&A to Revenue', 'Debt to Assets', \n        'Intangibles to Total Assets', 'R&D to Revenue']\n#These are closely linked to values we calculated above and thus nearest neighbour is apt\nknnalg = ['Average Inventory','Average Payables', 'Average Receivables', 'Book Value per Share',\n          'Capex per Share', 'Capex to Depreciation', 'Days Payables Outstanding', \n          'Days of Inventory on Hand','Inventory Turnover','Invested Capital', 'Market Cap', \n          'Net Current Asset Value','Payables Turnover']\n#We do not have enough information to closely match or calculate these already and thus use Imputer\nsimimp = ['Revenue per Share', 'Shareholders Equity per Share','EV to Operating cash flow',\n          'Stock-based compensation to Revenue','Tangible Book Value per Share', \n          'Capex to Operating Cash Flow','Capex to Revenue','Cash per Share',\n          'Days Sales Outstanding','EV to Free cash flow','EV to Sales','Earnings Yield',\n          'Enterprise Value','Enterprise Value over EBITDA','Free Cash Flow Yield', \n          'Free Cash Flow per Share','Graham Net-Net','Graham Number','Income Quality',\n          'Interest Coverage','Interest Debt per Share','Net Income per Share',\n          'Operating Cash Flow per Share','PB ratio','PE ratio','PFCF ratio', 'POCF ratio', \n          'PTB ratio','Price to Sales Ratio']",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Deal with noact and calc first\nfor i in noact:\n    alldata[i] = alldata[i].fillna(0)\nalldata.loc[alldata[calc[0]].isnull(), calc[0]] = alldata['Total liabilities'] / alldata['EBITDA']\nalldata.loc[alldata[calc[1]].isnull(), calc[1]] = alldata['Sales, General and administrative'] / alldata['Revenue']\nalldata.loc[alldata[calc[2]].isnull(), calc[2]] = alldata['Total liabilities'] / alldata['Total assets']\nalldata.loc[alldata[calc[3]].isnull(), calc[3]] = alldata['Intangible assets'] / alldata['Total assets']\nalldata.loc[alldata[calc[4]].isnull(), calc[4]] = alldata['Research and development'] / alldata['Revenue']",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Handle knnalg imputations\nrelevdata = alldata[np.append(alldata.columns.values[1:48], knnalg)]\nrelevdata = fast_knn(relevdata, k=5)\nalldata.drop(knnalg, axis=1, inplace=True)\nrelevdata = relevdata.iloc[:, 47:]\ncur = relevdata.columns.values\nrelevdata.rename(columns=dict(zip(cur, knnalg)), inplace=True)\nalldata = pd.concat([alldata, relevdata], axis=1)",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Explore feasible method of simple imputation based on value distribution\n#Here, we choose three random variables and plot these to get a general idea\nplt.subplot(311)\nplt.hist(alldata[simimp[0]].sort_values().dropna().values, bins=15, color='green')\nplt.title(simimp[0])\nplt.show()\nplt.subplot(312)\nplt.hist(alldata[simimp[4]].sort_values().dropna().values, bins=15, color='green')\nplt.title(simimp[4])\nplt.show()\nplt.subplot(313)\nplt.hist(alldata[simimp[10]].sort_values().dropna().values, bins=15, color='green')\nplt.title(simimp[10])\nplt.show()",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f7112b07e48>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f7113efc6d8>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f7112ead4a8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Since the distributions are clearly right-skewed, using 'mean' approach to impute missing values would likely lead to general over-estimates. Thus, since the data is not normal, we have two approaches: either power-transform the data into a normal distribution and then impute with mean, or just directly impute with median. To preserve the actual current values for the variables, the latter approach has been chosen. Therefore, scikitlearn's Imputer is used with the 'median' strategy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "imp = Imputer(missing_values=np.nan, strategy='median')\nfor col in simimp:\n    vals = alldata[col].values.reshape(-1,1)\n    newvals = imp.fit_transform(vals).reshape(1,-1)\n    alldata[col] = newvals[0]",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#All missing values have been successfully imputed\n#Save cleaned file before proceeding to feature engineering\nalldata.to_pickle('sp500cleandata.pkl')\nalldata.apply(lambda s: sum(s.isnull()) if sum(s.isnull()) > 0 else np.nan).dropna()",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "Series([], dtype: float64)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h2> Feature Engineering </h2>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this section, we engineer three varied buckets of features for the data - dividend values and attributes (amount, frequency and distance from payment), company characteristics (composition in the SP500 or industry on the basis of market cap etc.), and numerous financial ratios. This helps balance 'static' and 'dynamic' variables in our final data,  making it more representative and predictive of our response variable. After feature engineering, we proceed to model training."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 1: Dividend-Based Features"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Dividends can be calculated as either Payout Ratio * Earnings per Share or Dividend Yield * Stock Price. While these mathematically-yield close results, they cause a small an average difference of 0.13 cents on our data. Hence, the dividend value used in model training is the average of these two approaches. Subsequently, after dynamic dividend features are engineered, the payout ratio and yield are dropped as these cannot be known prior to dividend declaration and thus have no net predictive or applicable value in our pipeline."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata = pd.read_pickle('sp500cleandata.pkl')",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = alldata['Dividend Yield'] * alldata['stockprc']\ny = alldata['Payout Ratio'] * alldata['Basic_EPS']\n#Average error in two approaches to dividend\nprint(sum(x-y)/len(x))\nalldata['Dividend'] = (x + y)/2",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-0.09035920830895001\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata.Dividend.apply(lambda s: 'PAID'  if s > 0 else 'UNPAID').value_counts(dropna=False)",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 72,
          "data": {
            "text/plain": "PAID      1873\nUNPAID     451\nName: Dividend, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Dynamic payment duration variables\nalldata['Div_Paid?'] = alldata.Dividend.apply(lambda s: 1 if s > 0 else 0)\n#When attempting to check whether the company has paid dividend in the previous year, we would have to drop the first recording for\n#every firm since such data would be unavailable. However, since we already have limited data available from the API, it would \n#significantly reduce our training set and affect the applicability of the model. To get around it, we derive estimations for this\n#first recording. As such, we assign a 1 (last dividend paid) if the company has paid a special majority of more dividends \n#than not in recorded history, else 0. The reason we are stricter at a 0.67 requirement than 0.5 is to ensure greater certainty in\n#the fundamental dividend features in our model\nrow = np.cumsum(pd.Series(np.ones(len(alldata))))\ntrack = np.cumsum(alldata['Div_Paid?'])\nrow = row - row.where(alldata.firm != alldata.firm.shift(-1)).shift(1).ffill().fillna(0)\ntrack = track - track.where(alldata.firm != alldata.firm.shift(-1)).shift(1).ffill().fillna(0)\nrow = row.where(alldata.firm != alldata.firm.shift(-1)).bfill()\ntrack = track.where(alldata.firm != alldata.firm.shift(-1)).bfill()\ntrack = track/row\ntrack = track.where(alldata.firm != alldata.firm.shift(1)).fillna(0)\ntrack = track.apply(lambda s: 1 if s > 0.67 else 0)\nalldata['Paid_LastYr?'] = alldata['Div_Paid?'].shift(1)\nalldata.loc[alldata.firm != alldata.firm.shift(1), 'Paid_LastYr?'] = 0\nalldata['Paid_LastYr?'] = alldata['Paid_LastYr?'] + track",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#We now define a categorical variable - distance from last dividend. It can either be 1-5 or UNKNOWN. The reason this is being treated\n#as discrete not continuous is to allow the fact that since companies have existed for long durations, imputing unavailable data may lead\n#to excessive engineering of the training set, not being representative of the original data itself.\ndist = alldata['Paid_LastYr?'].map({1:0, 0:1})\ndist = np.cumsum(dist)\ndist = dist - dist.where(alldata.firm != alldata.firm.shift(-1)).shift(1).ffill().fillna(0)\nalldata['DistanceFromLast'] = dist + 1\nalldata['DistanceFromLast'] = alldata['DistanceFromLast'].astype('int').astype('str')\nalldata.loc[alldata.firm != alldata.firm.shift(1), \"DistanceFromLast\"] = 'UNKNOWN'",
      "execution_count": 74,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now try to find the proportion of industry that paid last year. Since the bulk of our data lies in 2014-2018, these years have enough samples to provide reliable estimates for this value. However, 2013 and 2019, seen below, have very few samples. Thus, we impute 2014's readings to 2013, and 2018's readings to 2019. This should not severely affect the model, given only 13 recordings total in consideration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata['Year'] = alldata['date'].apply(lambda s: int(s/100))\nalldata.drop(['date'], axis=1, inplace=True)\nalldata.Year.value_counts()",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 75,
          "data": {
            "text/plain": "2017    486\n2016    473\n2018    463\n2015    458\n2014    431\n2013      8\n2019      5\nName: Year, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#We first create a table with the year and sector wise cumulations of paid_lastyr? to determine prop of sector that paid\nvals = alldata[['Year', 'Sector', 'Paid_LastYr?']]\nvals.sort_values(by=['Sector', 'Year'], ascending=True, inplace=True)\nvals.reset_index(drop=True, inplace=True)\n#Since the data is first sorted on sector, we can use year as the divider between individual cumulative sums\nvals['Total'] = pd.Series(np.arange(len(vals)) + 1)\nvals['Total'] = vals['Total'] - vals['Total'].where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\ntmp = np.cumsum(vals['Paid_LastYr?'])\ntmp = tmp - tmp.where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\nvals['Paid_LastYr?'] = tmp\nvals['SectPaidLastYr'] = vals['Paid_LastYr?'] / vals['Total']\nvals.drop(['Paid_LastYr?', 'Total'], axis=1, inplace=True)\nvals.drop_duplicates(subset=['Sector', 'Year'], keep='last', inplace=True)\nvals.loc[vals.Year==2013, 'SectPaidLastYr'] = vals.SectPaidLastYr.shift(-1)\nvals.loc[vals.Year==2019, 'SectPaidLastYr'] = vals.SectPaidLastYr.shift(1)\n#Using the above table, we add the feature to alldata\nalldata = alldata.merge(vals, on=['Sector', 'Year'], how='left')",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  app.launch_new_instance()\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/pandas/util/decorators.py:91: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  return func(*args, **kwargs)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 2: Company Characteristics </b>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There is simply one major company characteristics we calculate based on market cap: the proportion of a company to average in its industry."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#We first create a table with the year and sector wise cumulations of market cap to determine ratio of company to avg yearly mktcap\nvals = alldata[['Year', 'Sector', 'Market Cap']]\nvals.sort_values(by=['Sector', 'Year'], ascending=True, inplace=True)\nvals.reset_index(drop=True, inplace=True)\n#Since the data is first sorted on sector, we can use year as the divider between individual cumulative sums\nvals['Total'] = pd.Series(np.arange(len(vals)) + 1)\nvals['Total'] = vals['Total'] - vals['Total'].where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\ntmp = np.cumsum(vals['Market Cap'])\ntmp = tmp - tmp.where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\nvals['Market Cap'] = tmp\nvals['AvgMktCap'] = vals['Market Cap'] / vals['Total']\nvals.drop(['Market Cap', 'Total'], axis=1, inplace=True)\nvals.drop_duplicates(subset=['Sector', 'Year'], keep='last', inplace=True)\nvals.loc[vals.Year==2013, 'AvgMktCap'] = vals.AvgMktCap.shift(-1)\nvals.loc[vals.Year==2019, 'AvgMktCap'] = vals.AvgMktCap.shift(1)\nalldata = alldata.merge(vals, on=['Sector', 'Year'], how='left')\nalldata['PropSectorAvgMktCap'] = alldata['Market Cap']/alldata['AvgMktCap']\nalldata.drop(['AvgMktCap'], axis=1, inplace=True)",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  app.launch_new_instance()\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/pandas/util/decorators.py:91: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  return func(*args, **kwargs)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<b> STEP 3: Financial Ratios </b>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We proceed to engineer limited financial ratios from the data available above. These help add greater depth to the variety of financial information available."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata['currentRatio'] = alldata['Total current assets'] / alldata['Total current liabilities']\nalldata['grossProfitMargin'] = alldata['Gross profit'] / alldata['Revenue']\nalldata['operatingProfitMargin'] = alldata['Operating income'] / alldata['Revenue']\nalldata['returnOnAssets'] = alldata['Net income'] / alldata['Total assets']\nalldata['assetTurnover'] = alldata['Revenue'] / alldata['Total assets']\nalldata['PBratio'] = (alldata['stockprc'] * alldata['Net income'] / alldata['Basic_EPS']) / (alldata['Total assets'] - alldata['Total liabilities'])",
      "execution_count": 78,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Drop errenous values\n#Due to the imputation and engineering performed in this section, there are a few cases where data is excessively manipulated such that\n#it does not hold logical coherence any more. We did not exclude them before feature engineering as we needed them for sector-related\n#feature engineering. These data points are eliminated below\nfirm = set(alldata.loc[alldata['Cost of revenue'] > alldata['Revenue'], \"firm\"])\nfirm = firm.union(set(alldata.loc[alldata['Total current liabilities'] > alldata['Total liabilities'], \"firm\"]))\nfirm = firm.union(set(alldata.loc[alldata['Total non-current liabilities'] > alldata['Total liabilities'], \"firm\"]))\nalldata = alldata[(~(alldata.firm.isin(firm)))]\nalldata.sort_values(by=['firm', 'Year'], ascending=[True, True], inplace=True)\nalldata.reset_index(drop=True, inplace=True)\nalldata.info()\n",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2127 entries, 0 to 2126\nColumns: 119 entries, Revenue to PBratio\ndtypes: float64(114), int64(2), object(3)\nmemory usage: 1.9+ MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alldata.to_pickle('sp500finaldata.pkl')",
      "execution_count": 80,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
